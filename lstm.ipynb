{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import *\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotConf(conf):\n",
    "    df_cm = pd.DataFrame(conf, index = [i for i in \"ABCDE\"],\n",
    "                  columns = [i for i in \"ABCDE\"])\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrain(X_trainIn,y_train,high):\n",
    "    #Creates a 3D array of numSamples X timepoints X features for training data\n",
    "    \n",
    "    X_train = X_trainIn.loc[:, ~X_trainIn.columns.isin(['num_window'])]\n",
    "    numFeatures = len(X_train.columns)\n",
    "    trainWindows = X_trainIn.groupby('num_window')['num_window'].unique()\n",
    "    trainArr = np.ndarray([len(trainWindows),int(high[0]), numFeatures])\n",
    "    yTrainArr = np.ndarray([len(trainWindows),1])\n",
    "\n",
    "\n",
    "    for i in range(len(trainWindows)):\n",
    "        blankDF = pd.DataFrame(columns=X_train.columns)\n",
    "        sample = X_train.loc[X_trainIn['num_window'] == trainWindows.index[i]]\n",
    "\n",
    "        \n",
    "        numberToPad = int(np.abs(len(sample)-high[0]))\n",
    "\n",
    "        for j in range(numberToPad):\n",
    "            blankDF.loc[j] = [0] * numFeatures\n",
    "        sample = sample.append(blankDF)\n",
    "\n",
    "        trainArr[i] = sample\n",
    "        yTrainArr[i] = y_train.loc[X_trainIn['num_window'] == trainWindows.index[i]].iloc[0]\n",
    "    \n",
    "    \n",
    "    return trainArr, yTrainArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTest(X_testIn, y_test, high):\n",
    "    #Creates a 3D array of numSamples X timepoints X features for training data\n",
    "    X_test = X_testIn.loc[:, ~X_testIn.columns.isin(['num_window'])]\n",
    "    numFeatures = len(X_test.columns)\n",
    "    testWindows = X_testIn.groupby('num_window')['num_window'].unique()\n",
    "    testArr = np.ndarray([len(testWindows),int(high[0]), numFeatures])\n",
    "    yTestArr = np.ndarray([len(testWindows),1])\n",
    "\n",
    "\n",
    "    for i in range(len(testWindows)):\n",
    "        blankDF = pd.DataFrame(columns=X_test.columns)\n",
    "        sample = X_test.loc[X_testIn['num_window'] == testWindows.index[i]]\n",
    "        numberToPad = int(np.abs(len(sample)-high[0]))\n",
    "\n",
    "        for j in range(numberToPad):\n",
    "            blankDF.loc[j] = [0] * numFeatures\n",
    "        sample = sample.append(blankDF)\n",
    "\n",
    "        testArr[i] = sample\n",
    "        yTestArr[i] = y_test.loc[X_testIn['num_window'] == testWindows.index[i]].iloc[0]\n",
    "    \n",
    "    \n",
    "    return testArr, yTestArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmModel(trainArr, yTrainArr): \n",
    "    n_timesteps, n_features, n_outputs = trainArr.shape[1], trainArr.shape[2], yTrainArr.shape[1]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"fs_raw_logo.csv\"\n",
    "df = read_csv(filename, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This chunk of code removes samples that don't have a certain number of rows\n",
    "#that correspond to time-points\n",
    "#Keep all rows with values between 25th-75th percentiles\n",
    "#We can pad these ones, but it would be too much to pad the others\n",
    "\n",
    "numWindowsPerSample = df.groupby('num_window')['num_window'].count()\n",
    "low = numWindowsPerSample.quantile([0.25]).values\n",
    "high = numWindowsPerSample.quantile([0.75]).values\n",
    "\n",
    "print(low)\n",
    "print(high)\n",
    "atLow = numWindowsPerSample.values >= low\n",
    "atHigh = numWindowsPerSample.values <=high\n",
    "\n",
    "windowBounds = atLow & atHigh\n",
    "\n",
    "windowsToUse = numWindowsPerSample[windowBounds]\n",
    "\n",
    "df = df.loc[df['num_window'].isin(windowsToUse.index)]\n",
    "df.to_csv('window.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df.user_name\n",
    "window = df.num_window\n",
    "y = df.classe\n",
    "X = df.loc[:, ~df.columns.isin(['classe', 'user_name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_inds, test_inds = next(GroupShuffleSplit().split(X, groups=window))\n",
    "#X_train, X_test, y_train, y_test = X.iloc[train_inds], X.iloc[test_inds], y.iloc[train_inds], y.iloc[test_inds]\n",
    "\n",
    "#We want to try to improve leave-one-out accuracy so we will do training 6 times\n",
    "#Use sklearn leaveOneGroupOut to generate our train and test sets based on group\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "lossArr = []\n",
    "accArr = []\n",
    "confArr = []\n",
    "\n",
    "batch=20\n",
    "epochs=4\n",
    "\n",
    "for train_index, test_index in logo.split(X, y, names):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    trainArr, yTrainArr = createTrain(X_train, y_train, high)\n",
    "    testArr, yTestArr = createTest(X_test, y_test, high)\n",
    "\n",
    "    print(\"Train X shape: \",trainArr.shape)\n",
    "    print(\"Train y shape: \",yTrainArr.shape)\n",
    "    print(\"Test X shape: \",testArr.shape)\n",
    "    print(\"Test y shape: \",yTestArr.shape)\n",
    "    \n",
    "    y_trainArr = to_categorical(yTrainArr)\n",
    "    y_testArr = to_categorical(yTestArr)\n",
    "    print(\"New train y shape: \", y_trainArr.shape)\n",
    "    print(\"New test y shape: \", y_testArr.shape)\n",
    "    from keras.wrappers.scikit_learn import KerasClassifier\n",
    "    #we need to modify the batch size for each iteration because we are using stacked LSTMs and the batch size\n",
    "    #needs to be able to divide into the number of samples\n",
    "    lstm = lstmModel(trainArr, y_trainArr)\n",
    "    outModel = lstm.fit(trainArr, y_trainArr, epochs=epochs, validation_data=(testArr,y_testArr), batch_size=batch, verbose=1)\n",
    "    lossArr.append(outModel.history)\n",
    "    ypred = lstm.predict_classes(testArr,batch_size=batch)\n",
    "\n",
    "\n",
    "    y_testArr = (np.ndarray.nonzero(y_testArr)[1])\n",
    "    conf = confusion_matrix(y_testArr, ypred)\n",
    "    confArr.append(conf)\n",
    "    plotConf(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "loss=[0]*epochs\n",
    "valloss=[0]*epochs\n",
    "for i in range(6):\n",
    "    loss = [sum(x) for x in zip(loss, lossArr[i]['loss'])]\n",
    "    valloss = [sum(x) for x in zip(valloss, lossArr[i]['val_loss'])]\n",
    "\n",
    "\n",
    "loss = [i/6for i in loss]\n",
    "valloss = [i/6 for i in valloss]\n",
    "\n",
    "plt.plot(loss)\n",
    "plt.plot(valloss)\n",
    "plt.legend(['train','test'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.xticks(np.arange(4), ('1', '2', '3', 4,))\n",
    "plt.show()\n",
    "\n",
    "print(\"final training loss: \", loss[len(loss)-1])\n",
    "print(\"final validation loss: \", valloss[len(valloss)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "trainAcc=[0]*epochs\n",
    "valAcc=[0]*epochs\n",
    "for i in range(6):\n",
    "    trainAcc = [sum(x) for x in zip(trainAcc, lossArr[i]['acc'])]\n",
    "    valAcc = [sum(x) for x in zip(valAcc, lossArr[i]['val_acc'])]\n",
    "\n",
    "\n",
    "trainAcc = [i/6 for i in trainAcc]\n",
    "valAcc = [i/6 for i in valAcc]\n",
    "\n",
    "plt.plot(trainAcc)\n",
    "plt.plot(valAcc)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.legend(['train','test'])\n",
    "plt.xticks(np.arange(4), ('1', '2', '3', 4,))\n",
    "plt.show()\n",
    "\n",
    "print(\"final training acc: \", trainAcc[len(trainAcc)-1])\n",
    "print(\"final validation acc: \", valAcc[len(valAcc)-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
